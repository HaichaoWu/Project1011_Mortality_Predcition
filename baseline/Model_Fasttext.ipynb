{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATAPATH = './data/data_baseline_fasttext_starspace/'\n",
    "import glob\n",
    "files = glob.glob(DATAPATH+'*')\n",
    "labels_dict = {}\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    f.close()\n",
    "    labels = [line[-5:-1] for line in lines]\n",
    "    labels_dict[file[40:-4]] = labels\n",
    "#     f_w = open(file, 'w')\n",
    "#     for line in lines:\n",
    "#         if 'LIVE' in line[-9:]:\n",
    "#             change = '__label__LIVE\\n'\n",
    "#         else:\n",
    "#             change = '__label__DEAD\\n'\n",
    "\n",
    "#         new_line  = ''.join([line[:-9], change])\n",
    "#         f_w.write(new_line)\n",
    "#     f_w.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/data_baseline_fasttext_starspace/labels_splitted_data.pickle', 'wb') as f:\n",
    "    pickle.dump(labels_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/data_baseline_fasttext_starspace/labels_splitted_data.pickle', 'rb') as f:\n",
    "    labels_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def auc_calculation(data_name, prob_file_name, labels_dict):\n",
    "    true = (pd.Series(labels_dict[data_name]) == 'DEAD') *1\n",
    "    \n",
    "    with open(prob_file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    prob_dead = []\n",
    "    for line in lines:\n",
    "        label_1, prob_1,label_2, prob_2 = line[:-2].split(' ')\n",
    "        if 'DEAD' in label_1:\n",
    "            prob_dead.append(float(prob_1))\n",
    "        else:\n",
    "            prob_dead.append(float(prob_2))\n",
    "    return roc_auc_score(true, prob_dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATAPATH = './data/data_baseline_fasttext_starspace/'\n",
    "\n",
    "import glob\n",
    "files = glob.glob(DATAPATH+'*')\n",
    "\n",
    "model_time = '12h'\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_model = [file for file in files if model_time in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_model\n",
    "train_data = []\n",
    "\n",
    "def read_data(file_name, path = DATAPATH, label = labels_dict):\n",
    "    data = []\n",
    "    file = path+file_name\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    f.close()\n",
    "    label_model = np.array(labels_dict[file_name[:-4]]) == 'DEAD'\n",
    "    label_model = label_model*1\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        data.append({'text':line[:-15], 'label':label_model[i]})\n",
    "        i+=1\n",
    "    return data\n",
    "\n",
    "def data_loading(model_time):\n",
    "    train_data = read_data('train_'+model_time+'.txt')\n",
    "    test_data = read_data('test_'+model_time+'.txt')\n",
    "    validation_data = read_data('val_'+model_time+'.txt')\n",
    "    return train_data, test_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data, validation_data =  data_loading(model_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATAPATH = './data/data_cnn/'\n",
    "#data = np.load(DATAPATH+'all_data.npy')\n",
    "\n",
    "vocabulary = np.load(os.path.join(DATAPATH, 'voc_100.npy'))\n",
    "index = range(len(vocabulary))\n",
    "voca_dict = dict(zip(vocabulary, index))\n",
    "\n",
    "test_6h = np.load(os.path.join(DATAPATH,'test_6h.npy')).item()\n",
    "train_6h = np.load(os.path.join(DATAPATH,'train_6h.npy')).item()\n",
    "val_6h = np.load(os.path.join(DATAPATH,'val_6h.npy')).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def notes_combine(infor_all):\n",
    "    data = infor_all['DATA'] \n",
    "    notes_per_sub = []\n",
    "    max_len = []\n",
    "    for sub in data:\n",
    "        notes = []\n",
    "        for note in sub:\n",
    "            d = [item for sublist in note for item in sublist]\n",
    "            notes.append(d )\n",
    "            max_len.append(len(d))\n",
    "        notes_per_sub.append(notes)\n",
    "    del notes\n",
    "    del d\n",
    "    return notes_per_sub, max_len\n",
    "\n",
    "def notes_padding(notes_all, note_len):\n",
    "    notes_per_sub = []\n",
    "    for sub in notes_all:\n",
    "        notes = []\n",
    "        for note in sub:\n",
    "            num_padding = note_len - len(note)\n",
    "            new_sentence = note + [0] * num_padding\n",
    "            notes.append(new_sentence )\n",
    "        notes_per_sub.append(notes)\n",
    "    del notes\n",
    "    return notes_per_sub\n",
    "def data_dict_generate(label, data):\n",
    "    result = []\n",
    "    for (d, l) in zip(data, label):\n",
    "        #print(l)\n",
    "        result.append({'text':d, 'label':torch.LongTensor([int(l)])})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_notes_6h, len_test = notes_combine(test_6h)\n",
    "train_notes_6h , len_train= notes_combine(train_6h)\n",
    "val_notes_6h, len_val = notes_combine(val_6h)\n",
    "len_all = np.array(len_test + len_train + len_val)\n",
    "\n",
    "number_of_notes_train = [len(x) for x in train_notes_6h]\n",
    "index = np.argsort(np.array(number_of_notes_train))\n",
    "train_notes_6h= np.array(train_notes_6h)[list(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_notes_6h_padded = notes_padding(test_notes_6h, len_all.max())\n",
    "train_notes_6h_padded = notes_padding(train_notes_6h, len_all.max())\n",
    "val_notes_6h_padded = notes_padding(val_notes_6h, len_all.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = data_dict_generate(val_6h['MORTALITY_LABEL'], val_notes_6h_padded)\n",
    "train_data = data_dict_generate(np.array(train_6h['MORTALITY_LABEL'])[index], train_notes_6h_padded)\n",
    "test_data = data_dict_generate(test_6h['MORTALITY_LABEL'], test_notes_6h_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# % %matplotlib inline\n",
    "# plt.hist(len_all, bins= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels\n",
    "\n",
    "def pad_minibatch(vectors):\n",
    "    length = max([len(x) for x in vectors])\n",
    "    padded_v = []\n",
    "    note_len = len(vectors[0][0])\n",
    "    for before_padded in vectors:\n",
    "        #print(before_padded)\n",
    "        v = before_padded.copy()\n",
    "        #print(type(v))\n",
    "        for i in range(length - len(before_padded)):\n",
    "            v.append([0]*note_len)\n",
    "        padded_v.append(v)\n",
    "    return Variable(torch.from_numpy(np.array(padded_v)).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function outputs the accuracy on the dataset, we will use it during training.\n",
    "import sklearn.metrics as metrics\n",
    "def evaluate(model, loss_, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    labels_all = []\n",
    "    output_all = []\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = pad_minibatch(vectors)\n",
    "        labels = Variable(torch.stack(labels).squeeze())\n",
    "        \n",
    "        hidden = model.init_hidden()\n",
    "        output = model(vectors, hidden)\n",
    "        loss = loss_(output, labels)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        predicted = predicted.numpy()\n",
    "        labels = labels.data.numpy()\n",
    "        correct += (predicted == labels).sum()\n",
    "        labels_all += list(labels)\n",
    "        output_all += list(output.data.numpy())\n",
    "    auc = metrics.roc_auc_score(labels_all, output_all)\n",
    "    return loss, correct / float(total), auc\n",
    "\n",
    "# This function gives us the confusion matrix for all labels and the overall accuracy.\n",
    "def evaluate_confusion(model, data_iter, lstm):\n",
    "    model.eval()\n",
    "    correct_all = 0\n",
    "    correct = {}\n",
    "    for lab in easy_label_map:\n",
    "        correct[lab] = [0,0,0,0,0] #eses, esar, ptpt, ptbr, total\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = Variable(torch.stack(vectors).squeeze())\n",
    "        labels = Variable(torch.stack(labels).squeeze())\n",
    "        \n",
    "        if lstm:\n",
    "            hidden, c_t = model.init_hidden()\n",
    "            output, hidden = model(vectors, hidden, c_t)\n",
    "        else:\n",
    "            hidden = model.init_hidden()\n",
    "            output, hidden = model(vectors, hidden)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct_all += (predicted == labels).sum()\n",
    "        \n",
    "        for lab in easy_label_map:\n",
    "            inds = (labels[:] == easy_label_map[lab]).nonzero().squeeze()\n",
    "            for i in range(len(easy_label_map)):\n",
    "                tmp =  torch.ones(len(inds.size())).long()*i\n",
    "                correct[lab][i] += (predicted[inds] == tmp).sum()\n",
    "            correct[lab][-1] += inds.size(0)\n",
    "        \n",
    "        confusion = {}\n",
    "        for val in correct:\n",
    "            confusion[val] = {v:correct[val][i] for i, v in enumerate(easy_label_map)}\n",
    "        \n",
    "    return confusion, correct_all / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import logging\n",
    "\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.batch_size = config['batch_size']\n",
    "        n_classes = config['target_class']\n",
    "        output_channel = config['output_channel']\n",
    "        self.note_gru_hidden = config['note_gru_hidden']\n",
    "        self.bidirection_gru = config['bidirection_gru']\n",
    "        self.note_embedding = Convolutional_Embedding(config)\n",
    "        self.note_gru = nn.GRU(output_channel, self.note_gru_hidden, bidirectional= self.bidirection_gru)\n",
    "        if self.bidirection_gru:\n",
    "            self.lin_out = nn.Linear(self.note_gru_hidden * 2, n_classes)\n",
    "        else:\n",
    "            self.lin_out = nn.Linear(self.note_gru_hidden, n_classes)\n",
    "        #self.softmax_note = nn.Softmax()\n",
    "        \n",
    "    def forward(self, mini_batch, hidden_state):\n",
    "        num_of_notes, num_of_words, batch_size = mini_batch.size()\n",
    "        s = None\n",
    "        for i in range(num_of_notes):\n",
    "            _s = self.note_embedding(mini_batch[i,:,:].transpose(0,1))\n",
    "            if (s is None):\n",
    "                s = _s.unsqueeze(0)\n",
    "            else:\n",
    "                s = torch.cat((s,_s.unsqueeze(0)),0)\n",
    "        #print('CNN: ', s.size())\n",
    "                \n",
    "        #packed_s = torch.nn.utils.rnn.pack_padded_sequence(s, length, batch_first=True)\n",
    "        \n",
    "        # (seq_len, batch, input_size),  (num_layers * num_directions, batch, hidden_size)\n",
    "        out_note, _ =  self.note_gru(s, hidden_state) \n",
    "        #print('RNN: ',out_note.size())\n",
    "        x = out_note[-1,:,:].squeeze()\n",
    "        x = self.lin_out(x)\n",
    "        return F.softmax(x)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirection_gru == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.note_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.note_gru_hidden))\n",
    "\n",
    "        \n",
    "class Convolutional_Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Convolutional_Embedding, self).__init__()\n",
    "        words_dim = config['words_dim']\n",
    "        self.embed_mode = config['embed_mode']\n",
    "        \n",
    "        output_channel = config['output_channel'] #\n",
    "        filter_width = config['filter_width']\n",
    "        \n",
    "        \n",
    "        vocab_size = config['vocab_size']\n",
    "        self.word_embed = nn.Embedding(vocab_size, words_dim)\n",
    "        #self.static_word_embed = nn.Embedding(vocab_size, words_dim)\n",
    "        #self.nonstatic_word_embed = nn.Embedding(vocab_size, words_dim)\n",
    "\n",
    "        #self.static_word_embed.weight.requires_grad = False\n",
    " \n",
    "        input_channel = 1\n",
    "    \n",
    "        self.conv= nn.Conv2d(input_channel, output_channel, (filter_width, words_dim), padding=(filter_width - 1, 0))\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "        #n_hidden = output_channel \n",
    "\n",
    "        #self.combined_feature_vector = nn.Linear(n_hidden, n_hidden_conv )\n",
    "        #self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.embed_mode == 'random':\n",
    "            x = self.word_embed(x) \n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        x = F.tanh(x).squeeze(3)\n",
    "        #print(x.size())\n",
    "        x = F.max_pool1d(x, x.size(2))\n",
    "        #print(x.size())\n",
    "        x = x.squeeze(2)  # max-over-time pooling\n",
    "        # append external features and feed to fc\n",
    "        #x = F.tanh(self.combined_feature_vector(x))\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.hidden(x)\n",
    "        #print(x.size())\n",
    "        return x\n",
    "\n",
    "class Hierachical_Convolutional_Embedding(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(config, model, loss_, optim, training_iter, dev_iter):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    total_batches = int(len(train_data) / config['batch_size'])\n",
    "    while epoch <= config['num_epochs']:\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter)) \n",
    "        vectors = pad_minibatch(vectors)\n",
    "        labels = Variable(torch.stack(labels).squeeze())\n",
    "        model.zero_grad()\n",
    "        \n",
    "        \n",
    "        hidden = model.init_hidden()\n",
    "        output = model(vectors, hidden)\n",
    "\n",
    "        lossy = loss_(output, labels)\n",
    "        lossy.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            if epoch % 5 == 0 and epoch!=0:\n",
    "                print(\"Epoch %i; Step %i / %i; Train Loss %f; Val Loss: %f; Val acc %f; Val AUC %f\" \n",
    "                      %(epoch, step, total_batches,lossy.data[0],\\\n",
    "                        evaluate(model, loss_, dev_iter)))\n",
    "            epoch += 1\n",
    "        else:\n",
    "            print(\"Epoch %i; Step %i / %i; Train Loss %f\" %(epoch, step, total_batches ,lossy.data[0]))\n",
    "            \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build, initialize, and train model\n",
    "config = {'vocab_size': len(vocabulary),\n",
    "          'words_dim': 100,\n",
    "          'embed_mode': 'random',\n",
    "          'output_channel': 50,\n",
    "          'dropout':0.3,\n",
    "          'target_class':2,\n",
    "          'note_gru_hidden':20,\n",
    "          'bidirection_gru': False,\n",
    "          'batch_size': 8,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs':100,\n",
    "          'filter_width':20\n",
    "         }\n",
    "\n",
    "model = AttentionRNN(config)\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(train_data, config['batch_size'])\n",
    "\n",
    "dev_iter = eval_iter(val_data, config['batch_size'])\n",
    "\n",
    "#training_loop(config, model, loss, optimizer, training_iter, dev_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1; Step 1 / 939; Train Loss 0.691603\n",
      "Epoch 1; Step 2 / 939; Train Loss 0.689102\n",
      "Epoch 1; Step 3 / 939; Train Loss 0.708444\n",
      "Epoch 1; Step 4 / 939; Train Loss 0.689816\n",
      "Epoch 1; Step 5 / 939; Train Loss 0.716236\n",
      "Epoch 1; Step 6 / 939; Train Loss 0.680342\n",
      "Epoch 1; Step 7 / 939; Train Loss 0.688293\n",
      "Epoch 1; Step 8 / 939; Train Loss 0.694934\n",
      "Epoch 1; Step 9 / 939; Train Loss 0.682638\n",
      "Epoch 1; Step 10 / 939; Train Loss 0.675712\n",
      "Epoch 1; Step 11 / 939; Train Loss 0.705282\n",
      "Epoch 1; Step 12 / 939; Train Loss 0.682603\n",
      "Epoch 1; Step 13 / 939; Train Loss 0.692630\n",
      "Epoch 1; Step 14 / 939; Train Loss 0.721081\n",
      "Epoch 1; Step 15 / 939; Train Loss 0.676921\n",
      "Epoch 1; Step 16 / 939; Train Loss 0.725046\n",
      "Epoch 1; Step 17 / 939; Train Loss 0.684346\n",
      "Epoch 1; Step 18 / 939; Train Loss 0.689783\n",
      "Epoch 1; Step 19 / 939; Train Loss 0.702076\n",
      "Epoch 1; Step 20 / 939; Train Loss 0.687220\n",
      "Epoch 1; Step 21 / 939; Train Loss 0.715771\n",
      "Epoch 1; Step 22 / 939; Train Loss 0.683069\n",
      "Epoch 1; Step 23 / 939; Train Loss 0.707227\n",
      "Epoch 1; Step 24 / 939; Train Loss 0.690315\n",
      "Epoch 1; Step 25 / 939; Train Loss 0.679429\n",
      "Epoch 1; Step 26 / 939; Train Loss 0.706362\n",
      "Epoch 1; Step 27 / 939; Train Loss 0.682822\n",
      "Epoch 1; Step 28 / 939; Train Loss 0.696151\n",
      "Epoch 1; Step 29 / 939; Train Loss 0.695256\n",
      "Epoch 1; Step 30 / 939; Train Loss 0.669640\n",
      "Epoch 1; Step 31 / 939; Train Loss 0.689315\n",
      "Epoch 1; Step 32 / 939; Train Loss 0.700174\n",
      "Epoch 1; Step 33 / 939; Train Loss 0.698184\n",
      "Epoch 1; Step 34 / 939; Train Loss 0.679511\n",
      "Epoch 1; Step 35 / 939; Train Loss 0.697860\n",
      "Epoch 1; Step 36 / 939; Train Loss 0.680282\n",
      "Epoch 1; Step 37 / 939; Train Loss 0.698413\n",
      "Epoch 1; Step 38 / 939; Train Loss 0.689137\n",
      "Epoch 1; Step 39 / 939; Train Loss 0.698120\n",
      "Epoch 1; Step 40 / 939; Train Loss 0.691185\n",
      "Epoch 1; Step 41 / 939; Train Loss 0.668478\n",
      "Epoch 1; Step 42 / 939; Train Loss 0.695536\n",
      "Epoch 1; Step 43 / 939; Train Loss 0.695168\n",
      "Epoch 1; Step 44 / 939; Train Loss 0.678972\n",
      "Epoch 1; Step 45 / 939; Train Loss 0.708800\n",
      "Epoch 1; Step 46 / 939; Train Loss 0.702822\n",
      "Epoch 1; Step 47 / 939; Train Loss 0.705956\n",
      "Epoch 1; Step 48 / 939; Train Loss 0.716543\n",
      "Epoch 1; Step 49 / 939; Train Loss 0.697950\n",
      "Epoch 1; Step 50 / 939; Train Loss 0.696887\n",
      "Epoch 1; Step 51 / 939; Train Loss 0.698344\n",
      "Epoch 1; Step 52 / 939; Train Loss 0.703485\n",
      "Epoch 1; Step 53 / 939; Train Loss 0.684463\n",
      "Epoch 1; Step 54 / 939; Train Loss 0.677792\n",
      "Epoch 1; Step 55 / 939; Train Loss 0.697426\n",
      "Epoch 1; Step 56 / 939; Train Loss 0.688316\n",
      "Epoch 1; Step 57 / 939; Train Loss 0.701604\n",
      "Epoch 1; Step 58 / 939; Train Loss 0.693617\n",
      "Epoch 1; Step 59 / 939; Train Loss 0.702098\n",
      "Epoch 1; Step 60 / 939; Train Loss 0.704958\n",
      "Epoch 1; Step 61 / 939; Train Loss 0.683410\n",
      "Epoch 1; Step 62 / 939; Train Loss 0.697665\n",
      "Epoch 1; Step 63 / 939; Train Loss 0.702455\n",
      "Epoch 1; Step 64 / 939; Train Loss 0.699190\n",
      "Epoch 1; Step 65 / 939; Train Loss 0.699937\n",
      "Epoch 1; Step 66 / 939; Train Loss 0.692478\n",
      "Epoch 1; Step 67 / 939; Train Loss 0.705887\n",
      "Epoch 1; Step 68 / 939; Train Loss 0.713019\n",
      "Epoch 1; Step 69 / 939; Train Loss 0.696503\n",
      "Epoch 1; Step 70 / 939; Train Loss 0.685285\n",
      "Epoch 1; Step 71 / 939; Train Loss 0.699039\n",
      "Epoch 1; Step 72 / 939; Train Loss 0.704773\n",
      "Epoch 1; Step 73 / 939; Train Loss 0.699683\n",
      "Epoch 1; Step 74 / 939; Train Loss 0.690906\n",
      "Epoch 1; Step 75 / 939; Train Loss 0.683620\n",
      "Epoch 1; Step 76 / 939; Train Loss 0.683443\n",
      "Epoch 1; Step 77 / 939; Train Loss 0.694718\n",
      "Epoch 1; Step 78 / 939; Train Loss 0.697912\n",
      "Epoch 1; Step 79 / 939; Train Loss 0.698779\n",
      "Epoch 1; Step 80 / 939; Train Loss 0.689137\n",
      "Epoch 1; Step 81 / 939; Train Loss 0.701083\n",
      "Epoch 1; Step 82 / 939; Train Loss 0.696126\n",
      "Epoch 1; Step 83 / 939; Train Loss 0.701155\n",
      "Epoch 1; Step 84 / 939; Train Loss 0.680552\n",
      "Epoch 1; Step 85 / 939; Train Loss 0.675757\n",
      "Epoch 1; Step 86 / 939; Train Loss 0.682239\n",
      "Epoch 1; Step 87 / 939; Train Loss 0.699949\n",
      "Epoch 1; Step 88 / 939; Train Loss 0.693482\n",
      "Epoch 1; Step 89 / 939; Train Loss 0.702485\n",
      "Epoch 1; Step 90 / 939; Train Loss 0.688815\n",
      "Epoch 1; Step 91 / 939; Train Loss 0.700540\n",
      "Epoch 1; Step 92 / 939; Train Loss 0.702722\n",
      "Epoch 1; Step 93 / 939; Train Loss 0.691221\n",
      "Epoch 1; Step 94 / 939; Train Loss 0.688906\n",
      "Epoch 1; Step 95 / 939; Train Loss 0.700931\n",
      "Epoch 1; Step 96 / 939; Train Loss 0.715282\n",
      "Epoch 1; Step 97 / 939; Train Loss 0.682453\n",
      "Epoch 1; Step 98 / 939; Train Loss 0.690271\n",
      "Epoch 1; Step 99 / 939; Train Loss 0.731037\n",
      "Epoch 1; Step 100 / 939; Train Loss 0.699866\n",
      "Epoch 1; Step 101 / 939; Train Loss 0.690947\n",
      "Epoch 1; Step 102 / 939; Train Loss 0.688109\n",
      "Epoch 1; Step 103 / 939; Train Loss 0.688726\n",
      "Epoch 1; Step 104 / 939; Train Loss 0.710854\n",
      "Epoch 1; Step 105 / 939; Train Loss 0.704066\n",
      "Epoch 1; Step 106 / 939; Train Loss 0.685929\n",
      "Epoch 1; Step 107 / 939; Train Loss 0.676574\n",
      "Epoch 1; Step 108 / 939; Train Loss 0.692389\n",
      "Epoch 1; Step 109 / 939; Train Loss 0.688398\n",
      "Epoch 1; Step 110 / 939; Train Loss 0.698330\n",
      "Epoch 1; Step 111 / 939; Train Loss 0.693540\n",
      "Epoch 1; Step 112 / 939; Train Loss 0.682685\n",
      "Epoch 1; Step 113 / 939; Train Loss 0.699825\n",
      "Epoch 1; Step 114 / 939; Train Loss 0.700621\n",
      "Epoch 1; Step 115 / 939; Train Loss 0.687994\n",
      "Epoch 1; Step 116 / 939; Train Loss 0.689710\n",
      "Epoch 1; Step 117 / 939; Train Loss 0.699865\n",
      "Epoch 1; Step 118 / 939; Train Loss 0.701156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-330489125fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-847538614c34>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(config, model, loss_, optim, training_iter, dev_iter)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlossy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlossy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=config['learning_rate'])\n",
    "training_loop(config, model, loss, optimizer, training_iter, dev_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of AttentionRNN (\n",
       "  (note_embedding): Convolutional_Embedding (\n",
       "    (word_embed): Embedding(181443, 300)\n",
       "    (static_word_embed): Embedding(181443, 300)\n",
       "    (nonstatic_word_embed): Embedding(181443, 300)\n",
       "    (conv): Conv2d(1, 100, kernel_size=(10, 300), stride=(1, 1), padding=(9, 0))\n",
       "    (dropout): Dropout (p = 0)\n",
       "  )\n",
       "  (note_gru): GRU(100, 20)\n",
       "  (lin_out): Linear (100 -> 2)\n",
       ")>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(out,truth):\n",
    "    def sm(mat):\n",
    "        exp = torch.exp(mat)\n",
    "        sum_exp = exp.sum(1,True)+0.0001\n",
    "        return exp/sum_exp.expand_as(exp)\n",
    "    #print(sm(out).size())\n",
    "    _,max_i = torch.max(sm(out),1)\n",
    "\n",
    "    eq = torch.eq(max_i,truth).float()\n",
    "    all_eq = torch.sum(eq)\n",
    "\n",
    "    return all_eq, all_eq/truth.size(0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-32ff6eaabd51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruth\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "truth = Variable(torch.from_numpy(np.array([0,1,0,1,1])))\n",
    "accuracy(final_out,truth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = truth.data.numpy()\n",
    "a == np.array([0,1,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(truth.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truth = Variable(torch.from_numpy(np.zeros((3,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = truth.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(a)+ list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0  0\n",
       " 0  0\n",
       " 0  0\n",
       "[torch.DoubleTensor of size 3x2]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.CrossEntropyLoss(torch.from_numpy(np.array([[0.3,0.7],[0.5,0.5],[0,1]])), torch.from_numpy(np.array([0,1,1])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss (\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
